---
title: "Exploracion"
date: "March 5, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# MNIST en formato CSV desde:
# https://pjreddie.com/projects/mnist-in-csv/
# mnist_train.csv (100Mb)
# mnist_test.csv (17Mb)

# MNIST con tidyverse desde
# https://www.r-bloggers.com/exploring-handwritten-digit-classification-a-tidy-analysis-of-the-mnist-dataset/
library(tidyverse)
library(rsample)
library(Metrics)
library(scales)
```


# Cargamos los datos

Los datos estan en los siguientes dos archivos.

```{r}
# Carguemos los archivos y luego escribamolos en un RData
#train_raw <- read_csv("data/mnist_train.csv", col_names = FALSE)
#test_raw <- read_csv("data/mnist_test.csv", col_names = FALSE)
#save(train_raw, test_raw, file = "data/mnist_data.RData")
# mnist_data.RData (20Mb)
# Sigamos cargando los datos desde mnist_data.RData
# que es mucho más rápido
load(file = "data/mnist_data.RData")
```

Exploremos la información

```{r}
str(train_raw,max.level = 0)
```

En el conjunto de train tenemos 60k de imágenes. Cada fila tiene 785 columnas donde la primera columna es el dígito en la imágen y las otras 784 son los 28x28 pixeles de cada imágen.

Cómo estan las proporciones de cada clase?

```{r}
train_raw %>% 
  ggplot(aes(X1)) +
  geom_bar() +
  labs(title = "Número de instancias por Número")
```

Es un dataset bastante balanciado en el número de instancias para cada clase.

```{r}
# Transformando los datos para visualización
pixels_gathered <- train_raw %>%
  head(10000) %>%
  rename(label = X1) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
pixels_gathered
```

```{r}
# Aprendiendo a visualizar imagenes en ggplot
pixels_gathered %>%
  filter(instance <= 12) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance + label)
```

Estas son las primeras 12 instancias del dataset con su respectiva clase de dígito.

Ahora hagamos la conversion de (0-255) a la escala (0-1) y pongamos la primera columna como un factor llamdo label

```{r,eval=FALSE}
train <- train_raw %>% 
  mutate_at(vars(X2:X785),funs(. / 255)) %>% 
  rename(label = X1) %>% 
  mutate(label = factor(label))
test <- test_raw %>% 
  mutate_at(vars(X2:X785),funs(. / 255)) %>% 
  rename(label = X1) %>% 
  mutate(label = factor(label))
```
```{r,include=FALSE}
load(file = "data/mnist_data_normalized.RData")
```


# Tiempos de Entrenamiento y Predicción

Cuando se trabaja con tantos datos, es recomendable muestrear los datos totales y trabajar con menos datos. Primero tratemos de evaluar los tiempos de entrenamiento y predicción para diferentes conjuntos de re-muestreo.

## SVM

Empecemos con un modeo de Máquinas de Soporte Vectorial.

### Pruebas usando datos en dataframe

Se demora 1.3 minutos entrenando un SVM con 6000 muestras. Se demora unos 30 segundos entrenando un modelo con 3000 muestras.

```{r, warning=FALSE, cache=TRUE}
# Ensayemos los tiempos de entrenamiento
d <- train %>% sample_n(3000)
start_time <- Sys.time()
mod <- kernlab::ksvm(label ~ . , data = d, C = 100)
end_time <- Sys.time()
end_time - start_time
```

Se demora 53 segundos prediciendo 6000 muestras con el modelo entrenado anteriormente. Se demora unos 20 segunos clasificando 3000 muestras.

```{r, cache=TRUE}
start_time <- Sys.time()
pred <- kernlab::predict(mod,(d %>% select(-label)))
end_time <- Sys.time()
end_time - start_time
```

### Pruebas usando datos en forma de matriz

Ahora ensayemos convirtiendo los dataframe a matrix primero. 

Con 6000 datos, se demora 1.2 minutos entrenando el SVM vs 1.3 minutos usando un dataframe. Se demora los mismo 30 segundos entrenando un modelo con 3000 datos.

```{r, warning=FALSE, cache=TRUE}
# ENsayemos los tiempos de entrenamiento
n <- 3000
d <- train %>% sample_n(n)
y <- d$label
x <- as.matrix(d %>% select(X2:X785))
start_time <- Sys.time()
mod <- kernlab::ksvm(x,y, C = 100)
end_time <- Sys.time()
end_time - start_time
```

Se demora los mismos 53 segundos prediciendo 6000 datos y los mismos 20 segundos para predecir 3000 datos

```{r,cache=TRUE}
start_time <- Sys.time()
pred <- kernlab::predict(mod,x)
end_time <- Sys.time()
end_time - start_time
```

# Modelos

## Creación de Dataframe para Validación Cruzada
Hagamos explicitos los dataframes de analysis y assessment

```{r}
# Empecemos con una version que use 5 pliegues sobre el 10% de los datos
v <- 2 # Número de Pliegues
r <- 2 # Número de Repeteciones
frac <- 0.1
set.seed(71265)
cv_data <- vfold_cv(train %>%
                      sample_frac(frac) ,
                    v = v,
                    repeats = r) %>% 
  mutate(analysis = map(splits,~analysis(.x)),
         assessment = map(splits,~assessment(.x)),
         y.assessment = map(assessment,~factor(.x$label)))
cv_data
```

## Configuración Procesamiento en Paralelo

Ensayemos hacer el procesamiento en paralelo. Esta forma de procesamiento en paralelo está explicada en el siguiente enlace:
https://www.r-bloggers.com/speed-up-your-code-parallel-processing-with-multidplyr/

Detectemos el número de cores

```{r}
library(parallel)
cl <- detectCores()
cl
```

Creemos un cluster con multidplyr

```{r}
library(multidplyr)
cluster <- create_cluster(cores = cl)
cluster
```

## Modelo con SVM

### Modelo Paralelizado

Ahora creemos un dataframe que tenga los valores de los parametros a probar

```{r}
modeloSVM <- cv_data %>% 
  crossing(C = c(10,100))
```

Asignemos los datos de cada pliegue a un core

```{r}
group <- rep(1:cl, length.out = nrow(modeloSVM))
modeloSVM <- bind_cols(tibble(group), modeloSVM)
```

Ahora creemos las particiones de los datos para cada cluster

```{r,warning=FALSE}
by_group <- modeloSVM %>%
    partition(group, cluster = cluster)
```

Parece como el tibble original, pero en realidad, el resultado es en formato party_df donde los datos estan separdos en 5 grupos de 4 (que es el número de procesadores)

Ahora, alistemos las funciones, variables y librerias que necesita cada uno de los componentes del cluster

```{r}
by_group %>%
    # Assign libraries
    cluster_library("tidyverse") %>%
    cluster_library("kernlab") %>%
    cluster_library("Metrics")
```

Verifiquemos que quedaron bien asigandas las librearias

```{r}
cluster_eval(by_group, search())[[1]]
```

Ahora, corramos el codigo en paralelo

```{r}
start <- proc.time() # Start clock
modeloSVM_parallel <- by_group %>% # Use by_group party_df
    mutate(modeloSVM = map2(analysis,C,~kernlab::ksvm(label ~ . , 
                                                 data = .x, 
                                                 C = .y)),
           y.pred = map2(modeloSVM,assessment, ~kernlab::predict(.x,.y)),
           validate_ce = map2_dbl(y.assessment,y.pred,~ce(.x,.y))) %>%
    collect() %>% # Special collect() function to recombine partitions
    as_tibble()   # Convert to tibble
time_elapsed_parallel <- proc.time() - start # End clock
time_elapsed_parallel
```

El mismo proceso se demoró 17 minutos con procesamiento en serie. En los 4 procesadores, el mismo proceso, se demoró solo 8 minutos.

Veamos el resultado

```{r}
if (r==1){
  results <- modeloSVM_parallel %>% 
    group_by(C) %>% 
    summarise(meanCE = mean(validate_ce))
} else {
  results <- modeloSVM_parallel %>% 
    group_by(C,id) %>% 
    summarise(meanCE = mean(validate_ce)) %>% 
    group_by(C) %>% 
    summarise(meanCE = mean(meanCE),
              var = var(meanCE))
}
results
```

Mejor C

```{r}
bestC <- results %>% 
  filter( meanCE == min(meanCE) ) %>% 
  .$C
bestC
```

### Modelo sin Paralelizar

Ahora veamos el mismo procesamiento en serie con un procesador

```{r, warning=FALSE}
# Validemos para 3 valores de C. 
# Este procesamiento tomó 17 minutos.
start_time <- Sys.time()
modeloSVM <- cv_data %>% 
  crossing(C = c(1,10,100)) %>% 
  mutate(modeloSVM = map2(analysis,C,~kernlab::ksvm(label ~ . , 
                                                 data = .x, 
                                                 C = .y)),
         y.pred = map2(modeloSVM,assessment, ~kernlab::predict(.x,.y)),
         validate_ce = map2_dbl(y.assessment,y.pred,~ce(.x,.y)))
end_time <- Sys.time()
end_time - start_time
```

Ahora se demoro 17 miuntos con el 10 porciento de los datos, 5 pliegues y la evalucion de 3 parametros (para un total del 15 modelos)

```{r}
modeloSVM %>% 
  group_by(C) %>% 
  summarize(meanCe = mean(validate_ce))
```


### Rendimiento en Test

Escojamos el mejor parametro C del modelo en entrenamiento y miremos como le va al modelo en el conjunto de prueba.

```{r}
bestC <- modeloSVM %>% 
  group_by(C) %>% 
  summarize(meanCe = mean(validate_ce)) %>% 
  ungroup() %>% 
  filter( meanCe == min(meanCe) ) %>% 
  .$C
bestC
```

Entrenemos un modelo sobre todos los datos de entrenamiento y miremos como le va en testing

```{r, warning=FALSE}
# Hagamos un modelo con una fracion de los datos de entrenamiento
d <- train %>% sample_frac(frac)
start_time <- Sys.time()
mod <- kernlab::ksvm(label ~ . , data = d, C = bestC)
pred <- kernlab::predict(mod,(test %>% select(-label)))
errorSVMTesting <- ce(test$label,pred)
end_time <- Sys.time()
end_time - start_time
```

Se demoró casi 3 minutos entrenando un modelo con el 10% de los datos y encontrando la tasa de clasificación incorrecta en las 10K imágenes del conjunto de prueba. Miremos el error de este modelo

```{r}
errorSVMTesting
```

No es un mal error sabiendo que el modelo se entrenos solo con el 10 porciento de los datos.

### Modelo Entrenado en la Nube

El código para escoger el mejor parametro y el modelo final esta en el archivo ModeloSVM.R

#### Entrenamiento en validación cruzada

A continuación estan los datos del modelo entrenado en la nube y guardado en el archivo: SVMTrain_v5_r2_frac10percent.RData

```{r}
#load("SVMTrain_v5_r2_frac10percent.RData")
load("SVMTrain_v5_r2_frac10percentRESULTS.RData")
trainSVMResults %>% 
  group_by(C,id) %>% 
  summarise(meanCE = mean(validate_ce)) %>% 
  ggplot(aes(C,meanCE)) +
  geom_point() +
  stat_summary(fun.y = mean, geom = "point", col = "red") +
  stat_summary(fun.y = mean, geom = "line", col = "red", aes(group = 1)) +
  scale_x_log10() +
  labs(title = "Tasa Clasificación Incorrecta en 2 Repeticiones",
       x = "Valor Parametro C",
       y = "Tasa de Clasificación Incorrecta") +
  scale_y_continuous(labels = percent_format())
```

En el gráfico, la linea roja es la tasa de media de clasificación incorrecta entre las repeticiones. 
Como el entrenamiento del SVM en todo el conjunto de entrenamiento es tan costoso en memoria y computacionalmente, carguemos el modelo entrenado en Amazon EC usando todos los datos de entrenamiento. 

#### Validación

Ahora miremos el tiempo de entrenamiento y error del modelo entrenado en la nube con todos los datos y el parametro C en 100.

```{r}
load("SVMTest_C100_allData.RData")
# Miremos primero cuanto se demoro en entrenar
test_time
```

Esto quiere decir que se demoro unos 32 minutos el entrenamiento. Lo que es aproximademente 10 veces más de lo que nos demoramos entrenando el mismo modelo con el 10% de los datos. Las pruebas de entrenamiento del modelo con todos los datos en un portatil convencionaal no fue exitoso por la gran cantidad de memoria necesaria.

Ahora miremos el error en el conjunto de prueba

```{r}
errorSVMTesting
```

Este modelo obtuvo un error del 1.76% de tasa de clasificación incorrecta.

## Modelo con GLM Multinomial

Hagamos un modelo GLM Multinomial con penalizacion elástica usando glment. 

### Tiempo Entrenamiento y Prueba en Subconjunto

Probemos cuanto se demora con el 5% de los datos y usando Lasso (alpha=1)

```{r,eval=FALSE}
n <- 3000
d <- train %>% sample_n(n)
start_glm <- Sys.time()
modGLM <- glmnetUtils::glmnet(label ~ . , data = d, family = "multinomial", alpha = 1)
end_glm <- Sys.time()
glm_train_time <-  end_glm - start_glm
glm_train_time
```
```{r,include=FALSE}
#save(modGLM,glm_train_time,file = "cache/glm_tiempo_3000.RData")
load("cache/glm_tiempo_3000.RData")
glm_train_time
```

Ahora como podemos predecir

```{r,eval=FALSE}
start_glm <- Sys.time()
y.pred <- predict(modGLM,test, type = "class", s=modGLM$lambda[1])
errorGLMTest <- ce(test$label,y.pred)
glm_test_time <-  Sys.time() - start_glm
glm_test_time
errorGLMTest
```
```{r,include=FALSE}
save(errorGLMTest,glm_test_time,file = "cache/glm_test_3000.RData")
load("cache/glm_test_3000.RData")
glm_test_time
errorGLMTest
```


Esto quiere decir que un modelo con todos los datos se puede demorar una media hora. 

### Entrenemos en Validación Cruzada

Carguemos los datos que se usaron en el servidor para que sirvan de base para el resto de modelos.

```{r}
load("cv_data.RData")
cv_data
```

Estos datos corresponden a 5 pliegues y dos repeticiones usando el 10% de los datos.

Ahora entrenemos un GLM en estos datos para 3 valores de alpha (0,0.5,1). Esto nos da un modelo Ridge, otro entre Ridge y Lasso, y finalmente, uno con Lasso.

```{r,warning=FALSE,eval=FALSE}
library(parallel)
cl <- detectCores()
library(multidplyr)
cluster <- create_cluster(cores = cl)

# Agregemos los diferentes modelos a calcular
modeloGLM <- cv_data %>% 
  crossing(alpha = c(0,0.5,1))

group <- rep(1:cl, length.out = nrow(modeloGLM))
modeloGLM <- bind_cols(tibble(group), modeloGLM)

by_group <- modeloGLM %>%
    partition(group, cluster = cluster) 

by_group %>%
    cluster_library("tidyverse") %>%
    cluster_library("glmnet") %>%
    cluster_library("glmnetUtils") %>%
    cluster_library("Metrics")

start <- Sys.time() # Start clock
modeloGLM_parallel <- by_group %>% # Use by_group party_df
    mutate(modeloGLM = map2(analysis,alpha,~glmnetUtils::glmnet(label ~ . , 
                                                 data = .x,
                                                 family = "multinomial",
                                                 alpha = .y)),
           lambda = map_dbl(modeloGLM,~(.x$lambda[1])),
           y.pred = map2(modeloGLM,assessment, ~predict(.x,.y, type = "class", s = .x$lambda[1])),
           validate_ce = map2_dbl(y.assessment,y.pred,~ce(.x,.y))) %>%
    collect() %>%
    as_tibble()
time_GLM_parallel <- Sys.time() - start
time_GLM_parallel
```
```{r,include=FALSE}
#save(time_GLM_parallel,modeloGLM_parallel,file = "cache/trainModeloGLM.RData")
load("cache/trainModeloGLM.RData")
time_GLM_parallel
```

Le tomó una hora y 6 minutos calcular estos 30 modelos con 4 procesadores.

```{r}
modeloGLM_parallel2 <- modeloGLM_parallel %>%
  mutate(lambda = map_dbl(modeloGLM,~(.x$lambda[1])),
           y.pred = map2(modeloGLM,assessment, ~predict(.x,.y, type = "class", s = .x$lambda[1])),
           validate_ce = map2_dbl(y.assessment,y.pred,~ce(.x,.y))) %>% 
  select(id,id2,lambda,validate_ce,alpha) %>% 
  arrange(id,id2,alpha)
```


Veamos el resultado

```{r}
results <- modeloGLM_parallel2 %>% 
  group_by(alpha,id) %>% 
  summarise(meanCE = mean(validate_ce),
            lambda = mean(lambda)) %>% 
  group_by(alpha) %>% 
  summarise(meanCE = mean(meanCE),
            lambda = mean(lambda))
results
```

Parece que no hubiera diferencia en el alpha. Escojamos un alpha de 0.5.

### Entrenamiento en Datos de Train

Ahora entrenemos un modelo GLM con estos parametros.

```{r,eval=FALSE}
start_glm <- Sys.time()
modGLM_All <- glmnetUtils::glmnet(label ~ . , data = train, family = "multinomial", alpha = 0.5)
glm_test_time <-  Sys.time() - start_glm
glm_test_time
```
```{r,include=FALSE}
#save(modGLM_All,glm_test_time,file = "cache/glm_test_all.RData")
load("cache/glm_test_all.RData")
glm_test_time
```

Se demoró una hora entrenando un modelo GLM en todo el conjunto de entrenamiento

### Rendimiento en Conjunto de Prueba

```{r,eval=FALSE}
y.pred <- predict(modGLM_All,test, type = "class", s=modGLM_All$lambda[1])
errorGLMTest_All <- ce(test$label,y.pred)
errorGLMTest_All
```
```{r,include=FALSE}
#save(errorGLMTest_All,glm_test_time,file = "cache/glm_pred_test_all.RData")
load("cache/glm_pred_test_all.RData")
errorGLMTest_All
```

El modelo obtuvo una tasa de clasificación incorrecta del 88.6% lo que lo pone muy por debajo del SVM.

## Modelo con CART

Ahora probemos con el arboles de clasificación. Empecemos usando "Gini"

```{r,eval=FALSE}
library(rpart)
start <- Sys.time()
modCARTGini <- rpart::rpart(label ~ . , data = train, method = "class", parms = list(split = "gini"))
cart_train_gini_time <-  Sys.time() - start
cart_train_gini_time
```
```{r,include=FALSE}
#save(modCARTGini,cart_train_gini_time,file = "cache/cart_train_gini.RData")
load("cache/cart_train_gini.RData")
cart_train_gini_time
```

Un poco mas de 3 minutos y medio para entrar en todo el conjunto de entrenamiento, lo que lo hace un método bastante rápido.

Ahora hagamos predición con este modelo sobre el conjunto de prueba.

```{r,eval=FALSE}
start <- Sys.time()
y.pred <- predict(modCARTGini,test, type = "class")
errorCARTGini <- ce(test$label,y.pred)
cart_pred_time_gini <-  Sys.time() - start
cart_pred_time_gini
errorCARTGini
```
```{r,include=FALSE}
save(errorCARTGini,cart_pred_time_gini,file = "cache/cart_pred_gini.RData")
load("cache/cart_pred_gini.RData")
cart_pred_time_gini
errorCARTGini
```

Se demoró 2 segundos prediciendo 10k datos de prueba pero obtuvo una tasa de clasificación incorrecta de solo el 38%

Ahora miremos como nos va usando "information".

```{r,eval=FALSE}
library(rpart)
start <- Sys.time()
modCARTInf <- rpart::rpart(label ~ . , data = train, method = "class", parms = list(split = "information"))
cart_train_inf_time <-  Sys.time() - start
cart_train_inf_time
```
```{r,include=FALSE}
#save(modCARTInf,cart_train_inf_time,file = "cache/cart_train_inf.RData")
load("cache/cart_train_inf.RData")
cart_train_inf_time
```

Un poco mas de 3 minutos y medio para entrar en todo el conjunto de entrenamiento.

Ahora hagamos predición con este modelo sobre el conjunto de prueba.

```{r,eval=FALSE}
start <- Sys.time()
y.pred <- predict(modCARTInf,test, type = "class")
errorCARTGInf <- ce(test$label,y.pred)
cart_pred_time_inf <-  Sys.time() - start
cart_pred_time_inf
errorCARTGInf
```
```{r,include=FALSE}
#save(errorCARTGInf,cart_pred_time_inf,file = "cache/cart_pred_inf.RData")
load("cache/cart_pred_inf.RData")
cart_pred_time_inf
errorCARTGInf
```

Se demoró solo segundo prediciendo pero con un tasa de calsificación incorrecta del 34%. Una tasa aun menor que usando "gini".

## Modelo con Random Forest

Probemos con el paquete "Ranger" que parece tener una implementación más veloz que la version original de "randomForest"

### Tiempo Entrenamiento y Prueba en Subconjunto

```{r,include=FALSE}
load("data/mnist_data_normalized.RData")
```
```{r,eval=FALSE}
library(ranger)
d <- train %>% sample_n(3000)
start <- Sys.time()
modRF3000 <- ranger::ranger(label ~ . , data = d, num.trees = 500)
rf_train_3000_time <-  Sys.time() - start
rf_train_3000_time
```
```{r,include=FALSE}
#save(modRF3000,rf_train_3000_time,file = "cache/rf_train_3000.RData")
load("cache/rf_train_3000.RData")
rf_train_3000_time
```

Se demoró 7 segundos entrenando 500 arboles. Es el mismo tiempo que se demoró en entrenar un árbol con CART.

Ahora hagamos predición con este modelo sobre el conjunto de prueba.

```{r,eval=FALSE}
start <- Sys.time()
y.pred <- predict(modRF3000,test, type = "response")
errorRF3000 <- ce(test$label,y.pred$predictions)
rf_pred_time_3000 <-  Sys.time() - start
rf_pred_time_3000
errorRF3000
```
```{r,include=FALSE}
#save(errorRF3000,rf_pred_time_3000,file = "cache/rf_pred_3000.RData")
load("cache/rf_pred_3000.RData")
rf_pred_time_3000
errorRF3000
```

Se demoró 1.8 segundos en predecir los 10k datos del conjunto de prueba y tuvo una tasa de clasificación incorrecta del 6.3%.

### Entrenemos en Validación Cruzada

Entrenamiento con 5 pliegues y 2 repeticiones para los parametros num.trees (100,250,500)

```{r,include=FALSE}
load("cv_data.RData")
```
```{r,warning=FALSE,eval=FALSE}
library(parallel)
cl <- detectCores()
library(multidplyr)
cluster <- create_cluster(cores = cl)

# Agregemos los diferentes modelos a calcular
modeloRF <- cv_data %>% 
  crossing(num.trees = c(100,250,500))

group <- rep(1:cl, length.out = nrow(modeloRF))
modeloRF <- bind_cols(tibble(group), modeloRF)

by_group <- modeloRF %>%
    partition(group, cluster = cluster) 

by_group %>%
    cluster_library("tidyverse") %>%
    cluster_library("ranger") %>%
    cluster_library("Metrics")

start <- Sys.time()
modeloRF_parallel <- by_group %>%
    mutate(modeloRF = map2(analysis,num.trees,~ranger::ranger(label ~ . , 
                                                 data = .x,
                                                 num.trees = .y)),
           y.pred = map2(modeloRF,assessment, ~predict(.x,.y, type = "response")),
           validate_ce = map2_dbl(y.assessment,y.pred,~ce(.x,(.y)$predictions))) %>%
    collect() %>%
    as_tibble()
time_RF_parallel <- Sys.time() - start
time_RF_parallel
```
```{r,include=FALSE}
#save(time_RF_parallel,modeloRF,file = "cache/trainModeloRF.RData")
load("cache/trainModeloRF.RData")
time_RF_parallel
```

Solo le tomó 5 minutos calcular estos 30 modelos con 4 procesadores.

Veamos el resultado

```{r}
results <- modeloRF_parallel %>% 
  group_by(num.trees,id) %>% 
  summarise(meanCE = mean(validate_ce)) %>% 
  group_by(num.trees) %>% 
  summarise(meanCE = mean(meanCE))
results
```

### Entrenamiento en Datos de Train

Ahora entrenemos un modelo de RandomForest con 500 arboles.

```{r,eval=FALSE}
start <- Sys.time()
modRF_All <- ranger::ranger(label ~ . , data = train, num.trees = 500)
rf_test_time <-  Sys.time() - start
rf_test_time
```
```{r,include=FALSE}
save(modRF_All,rf_test_time,file = "cache/rf_test_all.RData")
load("cache/rf_test_all.RData")
rf_test_time
```

Solo se demoró 5 minutos entrenando 500 arboles en los 60k de datos de entrenamiento. Impresionante.

### Rendimiento en Conjunto de Prueba

```{r,eval=FALSE}
start <- Sys.time()
y.pred <- predict(modRF_All,test, type = "response")
errorRFTest_All <- ce(test$label,y.pred$predictions)
rf_pred_test_time <-  Sys.time() - start
errorRFTest_All
rf_pred_test_time

```
```{r,include=FALSE}
#save(errorRFTest_All,rf_pred_test_time,file = "cache/rf_pred_test_all.RData")
load("cache/rf_pred_test_all.RData")
errorRFTest_All
rf_pred_test_time
```

El modelo obtuvo una tasa de clasificación incorrecta del 2.82% que es levemento menor al SVM. Le tomó 9 segundos en hacer la predicción de los 10k datos del conjunto de prueba on el modelo entrenado en los 60k de datos de entrenamiento.